# RIP JAKOB JOSEF
## Automatic ...
page 88:
- where do we get higher results? comparing with prev subtitles or by the professional?
    - [..]
  
page 89:
- crime series subtitles did best
- comedy (had a lot of puns and special featerus of language): also good
  - bc training material was comedy
  - these kinds of comedies, that come weekly, are highly repetitive
  - difficult puns did not perform that well
  - probably did not have many cultural references
- car document: not that good

### Last week: Neural Transform System: NMT
page 56:
2) in previous approaches: narrow context to left and right but here its considering the whole sentence all the time
3) if we have half the sentence already generated => also has an influence on the next word

page 57:
2) accuracy more of a problem than fluency
- problem of incompleteness solved
3) looks good even though there are some mistranslation

page 58:
- if evaluation is done by sentence by sentence: achieved
- but if paragraph per paragraph: human still better
- still true? has MT reached human parity?
  - deepl, google, textshuttle produce MT that are siglightly better than chatgpt
  - ..

page 59:
- how much time needed for post-editing
- predicted that around 2027 MT will reach human parity

### Machine Translation 4.0; Large Model baded Translation
page 64:
- suceeded in being best at latin-german translation
- but then with CHATgpt not anymore
  - plus it can also do to english
  - sort of like a mirical: bc they prob did not specifically train this
- not quite correct
  
page 67:
- first part in latin, seconf in German
- still performs well

page 69:
- suspect taht openai had added this as one of their task when training LLM for trasnlation
- then the system generalizes - what does it mean when i am asked to translate into a language

page 70:
- we dont know what training material was for chatgpt
- for palm it was opensource
- trained on 750 billion tokens
- 1.4% bilingual: top 2: french german
- there is bilinguak and pararell data
- how systems/LLM are larning bilingual properties

page 71:
- By adding translated data -> improve translation capabilities
- into english translation works best
  - dominant language for training material = english
  
page 72:
advantages of adding LLM on top of MT:
2) NMT still vrey bound to sentence to sentence translation
   
page 74:
- professional translators do this more - usually 1/2%
- and here just 0.5%
- more natural variation coming in

page 76:
- Gegeargumente:
  - cultural aspects

- PRoarguemte
  - huge time and money saver (teachers, etc.)
  - can strenghten small languages


#Intro to Information Retrival
### Keyword Search
- page rank algo
  - to get better search results
- ^basic idea of google:
  - if there are a lot of links to one page / visited often => important page
  
### Stopwords
- words that are not relevant for a text
- => frquent function words
  - can be removed without losing important information
- should keep as much as possible tho

### Average Ranked Precision
- indication for where are the documents with best matches
- often in exam!!!
  - should be able to comute this!!

# Benchmarking ChatGPT
  ### Benchmark Datasets:
  - worked on evaluating chatgpt
  - enourmous amount of tests
  - rouge score = similar to bleu score; wordoverlap; precison and recall based overlapp score
  - EThics Psychology:
    - give it the train problem/ ethical dillemas
  - Open domain Question Answering
    - like "how high is eiffel tower"
- Reading Comprehension:
  - having a text and then asnwering questions to it
- benchmark test was done, means:
    - ...
  - 

### SuperGlue
- entailment = lexical semantics



page 14:
- anything above 20 makes sende
- eng - kk the worst

### Ethics Benchmark
- how to measure ethics:
- chatgpt much better in terms of justice
- 
